{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Activation, Flatten, MaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import Hyperband\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enables Tensorflow to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "   tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\"pickels\": \"../../assets/audio_sentiment_data_v2/pickles\",\n",
    "         \"models\": \"../../assets/audio_sentiment_data_v2/models\",\n",
    "         \"tuners\": \"../../assets/audio_sentiment_data_v2/tuners\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data splits from the pickels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(f\"{paths['pickels']}/X_train.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(f\"{paths['pickels']}/X_val.pickle\",\"rb\")\n",
    "X_val = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(f\"{paths['pickels']}/X_test.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(f\"{paths['pickels']}/y_train.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(f\"{paths['pickels']}/y_val.pickle\",\"rb\")\n",
    "y_val = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(f\"{paths['pickels']}/y_test.pickle\",\"rb\")\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2445, 37, 1) (489, 37, 1) (326, 37, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model using HyperModel Class to perform hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNHyperModel(HyperModel):\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(\n",
    "            Conv1D(\n",
    "                filters=hp.Choice(\n",
    "                    'num_filters_1',\n",
    "                    values=[16, 32, 64],\n",
    "                    default=64,\n",
    "                ),\n",
    "                kernel_size=3,\n",
    "                activation=hp.Choice(\n",
    "                    'conv_activation_1',\n",
    "                    values=['relu', 'tanh'],\n",
    "                    default='relu'\n",
    "                ),\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add(\n",
    "            Conv1D(\n",
    "                filters=hp.Choice(\n",
    "                    'num_filters_2',\n",
    "                    values=[16, 32, 64],\n",
    "                    default=64,\n",
    "                ),\n",
    "                kernel_size=3,\n",
    "                activation=hp.Choice(\n",
    "                    'conv_activation_2',\n",
    "                    values=['relu', 'tanh'],\n",
    "                    default='relu'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        \n",
    "        model.add(\n",
    "            Dropout(rate=hp.Float(\n",
    "                'dropout_1',\n",
    "                min_value=0.0,\n",
    "                max_value=0.5,\n",
    "                default=0.25,\n",
    "                step=0.05,\n",
    "            ))\n",
    "        )\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(\n",
    "                    'dense_units',\n",
    "                    min_value=16,\n",
    "                    max_value=72,\n",
    "                    step=16,\n",
    "                    default=32\n",
    "                ),\n",
    "                activation=hp.Choice(\n",
    "                    'dense_activation',\n",
    "                    values=['relu', 'tanh', 'sigmoid'],\n",
    "                    default='relu'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add(\n",
    "            Dropout(rate=hp.Float(\n",
    "                'dropout_2',\n",
    "                min_value=0.0,\n",
    "                max_value=0.5,\n",
    "                default=0.25,\n",
    "                step=0.05,\n",
    "            ))\n",
    "        )\n",
    "        \n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                hp.Float(\n",
    "                    'learning_rate',\n",
    "                    min_value=1e-4,\n",
    "                    max_value=1e-2,\n",
    "                    sampling='LOG',\n",
    "                    default=1e-3\n",
    "                )\n",
    "            ),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an instance of the HyperModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5\n",
    "INPUT_SHAPE = (37, 1)\n",
    "\n",
    "hypermodel = CNNHyperModel(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Hyperband Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "hb_tuner = Hyperband(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=40,\n",
    "    hyperband_iterations=2,\n",
    "    executions_per_trial=2,\n",
    "    seed=SEED,\n",
    "    directory=f'{paths[\"tuners\"]}/hyperband_tuner',\n",
    "    project_name='AudioSentimentClassifier'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "hb_tuner.search(X_train,\n",
    "             y_train,\n",
    "             validation_data=(X_val, y_val),\n",
    "             epochs=40,\n",
    "             callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = hb_tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = hb_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hb_tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining with optimal epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypermodel = hb_tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "history = hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_data=(X_val, y_val), verbose=0)\n",
    "print(f\"val_acc = {history.history['val_accuracy']}\\nval_loss = {history.history['val_loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"hyperband_tuned_best_model_val_acc{history.history['val_accuracy']}_val_loss{history.history['val_loss']}.hdf5\"\n",
    "filepath = f\"{paths['models']}/{name}.hdf5\"\n",
    "hypermodel.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the RandomSearch Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "rs_tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    executions_per_trial=2,\n",
    "    directory=f'{paths[\"tuners\"]}/random_search_tuner',\n",
    "    project_name='AudioSentimentClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 33s]\n",
      "val_accuracy: 0.7770960927009583\n",
      "\n",
      "Best val_accuracy So Far: 0.8333333134651184\n",
      "Total elapsed time: 00h 10m 46s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "rs_tuner.search(X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 35, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 33, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 78,533\n",
      "Trainable params: 78,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model = rs_tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = rs_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tanh'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps.get('conv_activation_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 31\n"
     ]
    }
   ],
   "source": [
    "model = rs_tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining with optimal epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypermodel = rs_tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "history = hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_data=(X_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc = 0.8179959058761597\n",
      "val_loss = 0.4643225073814392\n"
     ]
    }
   ],
   "source": [
    "val_acc = history.history['val_accuracy']\n",
    "max_val_acc = max(val_acc)\n",
    "max_val_acc_loss = history.history['val_loss'][val_acc.index(max(val_acc))]\n",
    "print(f\"val_acc = {max_val_acc}\\nval_loss = {max_val_acc_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"random_search_tuned_best_model_val_acc{max_val_acc}_val_loss{max_val_acc_loss}.hdf5\"\n",
    "filepath = f\"{paths['models']}/{name}.hdf5\"\n",
    "hypermodel.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4291 - accuracy: 0.8436\n",
      "[test loss, test accuracy]: [0.4290520250797272, 0.8435583114624023]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "new_y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(new_y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"labels.pickle\",\"rb\")\n",
    "lb= pickle.load(pickle_in)\n",
    "labels = lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "conf_matrix = pd.DataFrame(tf.math.confusion_matrix(new_y_test, predictions).numpy(), index=labels, columns=labels)\n",
    "conf_matrix.index.name = 'Actual'\n",
    "conf_matrix.columns.name = 'Predicted'\n",
    "fig_dims = (8, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", annot_kws={\"size\": 16}, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beproj",
   "language": "python",
   "name": "beproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
