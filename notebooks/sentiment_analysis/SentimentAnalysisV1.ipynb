{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import keras\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "# import text2emotion as te\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_watson.websocket import AudioSource, RecognizeCallback\n",
    "from pydub import AudioSegment\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "   tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup STT Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"api_key\")\n",
    "url = os.getenv(\"url\")\n",
    "\n",
    "# Setup Service\n",
    "authenticator = IAMAuthenticator(api_key)\n",
    "stt = SpeechToTextV1(authenticator=authenticator)\n",
    "stt.set_service_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {'audiobooks': \"../../assets/audiobooks\",\n",
    "         'clips': \"../../assets/temp\",\n",
    "         \"pickels\": \"../../assets/audio_sentiment_data_v2/pickles\",\n",
    "         \"models\": \"../../assets/audio_sentiment_data_v2/models\",}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_convert_audiobook_to_wav():\n",
    "    # Fetch the .mp3 audiobook file\n",
    "    for file in os.listdir(paths['audiobooks']):\n",
    "        if os.path.splitext(file)[1] == '.mp3':\n",
    "            file_name = os.path.abspath(os.path.join(paths['audiobooks'], file))\n",
    "    \n",
    "    file_name_wav = file_name[:-3]+'wav'\n",
    "    \n",
    "    # Convert .mp3 to .wav\n",
    "    sound = AudioSegment.from_mp3(file_name)\n",
    "    sound.export(file_name_wav, format=\"wav\")\n",
    "    \n",
    "    return file_name_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_sentiment_analysis(file):\n",
    "    # Perform transcription\n",
    "    with open(file, 'rb') as f:\n",
    "        res = stt.recognize(audio=f, content_type='audio/wav', model='en-US_NarrowbandModel', continuous=True).get_result()\n",
    "\n",
    "    text = \"\"\n",
    "    conf = 0.0\n",
    "\n",
    "    for i in range(len(res['results'])):\n",
    "        text += res['results'][i]['alternatives'][0]['transcript'][0:-1] + \". \"\n",
    "        conf += res['results'][i]['alternatives'][0]['confidence']\n",
    "    trans_conf = conf/len(res['results'])\n",
    "    \n",
    "    # Get emotions from transcript\n",
    "    text_emotions = te.get_emotion(text)\n",
    "    print(text_emotions)\n",
    "    \n",
    "    # Convert to weighted text emotions (weight = 65%)\n",
    "    w_text_emotions = np.zeros(5)\n",
    "    for index, emotion in enumerate(sorted(text_emotions)):\n",
    "        w_text_emotions[index] =  float(text_emotions[emotion])\n",
    "    w_text_emotions = w_text_emotions*0.65\n",
    "    \n",
    "    return w_text_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_senti_model():\n",
    "    model_name = \"hyperband_tuned_model_final_[0.260879248380661, 0.9069767594337463]\"\n",
    "    model = tf.keras.models.load_model(f\"{paths['models']}/{model_name}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickles():\n",
    "    # Load in the labels and scaler from the pickles \n",
    "    pickles = {}\n",
    "    pickle_in = open(f\"{paths['pickels']}/labels.pickle\",\"rb\")\n",
    "    pickles['labels'] = pickle.load(pickle_in)\n",
    "    pickle_in = open(f\"{paths['pickels']}/scaler.pickle\",\"rb\")\n",
    "    pickles['scaler'] = pickle.load(pickle_in)\n",
    "    \n",
    "    return pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(y, sr):\n",
    "    rmse= np.mean(librosa.feature.rms(y=y))\n",
    "    spec_cent = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "    spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "    rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(y=y, sr=sr), axis=1)\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr), axis=1)\n",
    "\n",
    "    data_features = [rmse,\n",
    "                    spec_cent,\n",
    "                    spec_bw, \n",
    "                    rolloff, \n",
    "                    zcr, \n",
    "                    chroma_stft[0],\n",
    "                    chroma_stft[1],\n",
    "                    chroma_stft[2],\n",
    "                    chroma_stft[3],\n",
    "                    chroma_stft[4],\n",
    "                    chroma_stft[5],\n",
    "                    chroma_stft[6],\n",
    "                    chroma_stft[7],\n",
    "                    chroma_stft[8],\n",
    "                    chroma_stft[9],\n",
    "                    chroma_stft[10],\n",
    "                    chroma_stft[11],\n",
    "                    mfcc[0],\n",
    "                    mfcc[1],\n",
    "                    mfcc[2],\n",
    "                    mfcc[3],\n",
    "                    mfcc[4],\n",
    "                    mfcc[5],\n",
    "                    mfcc[6],\n",
    "                    mfcc[7],\n",
    "                    mfcc[8],\n",
    "                    mfcc[9],\n",
    "                    mfcc[10],\n",
    "                    mfcc[11],\n",
    "                    mfcc[12],\n",
    "                    mfcc[13],\n",
    "                    mfcc[14],\n",
    "                    mfcc[15],\n",
    "                    mfcc[16],\n",
    "                    mfcc[17],\n",
    "                    mfcc[18],\n",
    "                    mfcc[19]\n",
    "                    ]\n",
    "    return data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X, pickles):\n",
    "    scaler = pickles['scaler']\n",
    "    \n",
    "    return scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_sentiment_analysis(file, model, pickles):\n",
    "    audio, sr = librosa.load(file, res_type='kaiser_fast', sr=22050*2)\n",
    "    \n",
    "    buffer = 3 * sr\n",
    "\n",
    "    samples_total = len(audio)\n",
    "    samples_wrote = 0\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    while samples_wrote < samples_total:\n",
    "\n",
    "        #check if the buffer is not exceeding total samples \n",
    "        if buffer > (samples_total - samples_wrote):\n",
    "            buffer = samples_total - samples_wrote\n",
    "\n",
    "        block = audio[samples_wrote : (samples_wrote + buffer)]\n",
    "\n",
    "        data_features = np.array(feature_extraction(block, sr))\n",
    "\n",
    "        scaled_features = scale_features(data_features.reshape(1, -1), pickles)\n",
    "\n",
    "        predictions.append(model.predict(scaled_features))\n",
    "\n",
    "        samples_wrote += buffer\n",
    "        \n",
    "    audio_emotions = np.squeeze(predictions, axis=None)\n",
    "    audio_emotions_length = len(audio_emotions)\n",
    "    audio_emotions = audio_emotions.sum(axis=0)\n",
    "    audio_emotions = audio_emotions / audio_emotions_length\n",
    "    \n",
    "    audio_emotions = audio_emotions.argmax()\n",
    "    audio_emotions = audio_emotions.astype(int).flatten()\n",
    "    final_emotion = labels.inverse_transform((audio_emotions))\n",
    "    \n",
    "    return final_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    audio_senti_model = load_audio_senti_model()\n",
    "    pickles = load_pickles()\n",
    "    file = fetch_and_convert_audiobook_to_wav()\n",
    "    audio, sr = librosa.load(file, res_type='kaiser_fast', sr=22050*2)\n",
    "\n",
    "    emotions_list = []\n",
    "\n",
    "    # Splitting the audiobook into 30 sec clips\n",
    "    buffer = 30 * sr\n",
    "    samples_total = len(audio)\n",
    "    samples_wrote = 0\n",
    "    counter = 1\n",
    "\n",
    "    while samples_wrote < samples_total:\n",
    "\n",
    "        #check if the buffer is not exceeding total samples\n",
    "        if buffer > (samples_total - samples_wrote):\n",
    "            buffer = samples_total - samples_wrote\n",
    "\n",
    "        block = audio[samples_wrote : (samples_wrote + buffer)]\n",
    "        out_file_name = \"clip_\" + str(counter) + \".wav\"\n",
    "        complete_name = os.path.join(paths['clips'], out_file_name)\n",
    "\n",
    "        # Save the 30 sec clip\n",
    "        sf.write(complete_name, block, sr)\n",
    "\n",
    "        # Load the 30 sec clip\n",
    "        file = os.path.abspath(complete_name)\n",
    "\n",
    "        # Perform text sentiment analysis\n",
    "        w_text_emotions = text_sentiment_analysis(file)\n",
    "        print(\"Text Sentiment Analysis Completed.\")\n",
    "\n",
    "        # Perform audio sentiment analysis\n",
    "        w_audio_emotions = audio_sentiment_analysis(complete_name, audio_senti_model, pickles)\n",
    "        print(\"Audio Sentiment Analysis Completed.\")\n",
    "\n",
    "        # Add weighted emotions\n",
    "        weighted_emotions = w_text_emotions + w_audio_emotions\n",
    "\n",
    "        # Add emotion to emotions list\n",
    "        labels = pickles['labels']\n",
    "        weighted_emotions = weighted_emotions.argmax()\n",
    "        weighted_emotions = weighted_emotions.astype(int).flatten()\n",
    "        final_emotion = labels.inverse_transform((weighted_emotions))\n",
    "        emotions_list.append(final_emotion[0])\n",
    "\n",
    "        # Delete the clip once sentiment analysis is done\n",
    "        os.remove(complete_name)\n",
    "\n",
    "        counter += 1\n",
    "        samples_wrote += buffer\n",
    "        \n",
    "    return emotions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.4, 'Sad': 0.4, 'Fear': 0.2}\n",
      "Text Sentiment Analysis Completed.\n",
      "[0.24394405 0.15688714 0.09505951 0.17667837 0.3274309 ]\n",
      "Audio Sentiment Analysis Completed.\n",
      "{'Happy': 0.29, 'Angry': 0.0, 'Surprise': 0.24, 'Sad': 0.29, 'Fear': 0.18}\n",
      "Text Sentiment Analysis Completed.\n",
      "[0.10294437 0.18543912 0.09894231 0.47148782 0.14118639]\n",
      "Audio Sentiment Analysis Completed.\n",
      "{'Happy': 0.08, 'Angry': 0.0, 'Surprise': 0.5, 'Sad': 0.25, 'Fear': 0.17}\n",
      "Text Sentiment Analysis Completed.\n",
      "[0.0746337  0.2524653  0.16625951 0.23488536 0.27175602]\n",
      "Audio Sentiment Analysis Completed.\n",
      "{'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.11, 'Sad': 0.33, 'Fear': 0.56}\n",
      "Text Sentiment Analysis Completed.\n",
      "[0.27562848 0.36130312 0.069392   0.15604857 0.13762784]\n",
      "Audio Sentiment Analysis Completed.\n",
      "['surprise', 'sad', 'surprise', 'fear']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    emotions_list = main()\n",
    "    print(emotions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation of Audio sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danny\\miniconda3\\envs\\deepaudiobooktuner\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.24.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Danny\\miniconda3\\envs\\deepaudiobooktuner\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = load_audio_senti_model()\n",
    "pickles = load_pickles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(f\"{paths['pickels']}/X_val.pickle\",\"rb\")\n",
    "X_val = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(f\"{paths['pickels']}/y_val.pickle\",\"rb\")\n",
    "y_val = pickle.load(pickle_in)\n",
    "y_val = y_val.astype(int)\n",
    "\n",
    "pickle_in = open(f\"{paths['pickels']}/labels.pickle\",\"rb\")\n",
    "labels = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(y):\n",
    "    y = y.argmax()\n",
    "    y = y.astype(int).flatten()\n",
    "    final_emotion = labels.inverse_transform((y))\n",
    "    return final_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing validation clip 23\n",
      "Actual Emotion :happy\n",
      "Predicted Emotion = happy\n",
      "========================\n",
      "Testing validation clip 271\n",
      "Actual Emotion :happy\n",
      "Predicted Emotion = happy\n",
      "========================\n",
      "Testing validation clip 383\n",
      "Actual Emotion :neutral\n",
      "Predicted Emotion = neutral\n",
      "========================\n",
      "Testing validation clip 334\n",
      "Actual Emotion :neutral\n",
      "Predicted Emotion = neutral\n",
      "========================\n",
      "Testing validation clip 104\n",
      "Actual Emotion :sad\n",
      "Predicted Emotion = neutral\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "testlen=5\n",
    "for i in range(testlen):\n",
    "    randsample = random.randrange(0,385)\n",
    "    print(f\"Testing validation clip {randsample}\")\n",
    "    print(f'Actual Emotion :{get_emotion(y_val[randsample])[0]}')\n",
    "    prediction = model.predict(X_val[randsample].reshape(1, -1))\n",
    "    print(f\"Predicted Emotion = {get_emotion(prediction)[0]}\")\n",
    "    print(\"========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepaudiobooktuner",
   "language": "python",
   "name": "deepaudiobooktuner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
