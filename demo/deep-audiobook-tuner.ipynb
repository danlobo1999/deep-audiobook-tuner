{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "after-ready",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# deep-audiobook-tuner\n",
    "A system that generates an apt, emotionally pertinent, unique musical score for an audiobook automatically based on the current narrative for the purpose of ameliorating user-experience while being accurate, cost-efficient, and time saving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-jewel",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-macro",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import asyncio\n",
    "from ipywidgets import FileUpload\n",
    "from IPython.display import display, clear_output\n",
    "import keras\n",
    "import ktrain\n",
    "import librosa\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "from deepaudiobooktuner.audio_analysis import *\n",
    "from deepaudiobooktuner.ibm_transcription import *\n",
    "from deepaudiobooktuner.text_analysis import *\n",
    "from deepaudiobooktuner.music_generation import *\n",
    "from deepaudiobooktuner.utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "   tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-melissa",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "#### Uploading an audiobook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-joshua",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def wait_for_change(widget, value):\n",
    "    future = asyncio.Future()\n",
    "    def getvalue(change):\n",
    "        # make the new value available\n",
    "        future.set_result(change.new)\n",
    "        widget.unobserve(getvalue, value)\n",
    "    widget.observe(getvalue, value)\n",
    "    return future\n",
    "\n",
    "async def uploadAudiobook():\n",
    "    # displaying an upload widget\n",
    "    upload = FileUpload(accept='.mp3', multiple=False)\n",
    "    display(upload)\n",
    "    \n",
    "    future = await wait_for_change(upload, 'value')\n",
    "    \n",
    "    uploaded_filename = list(upload.value)[0]\n",
    "    audiobook_path = f'{path(\"../assets/audiobooks\")}/{uploaded_filename}'\n",
    "    \n",
    "    # Saving the audiobook to the audiobook directory\n",
    "    with open(f'{audiobook_path}', 'wb') as output_file: \n",
    "        content = upload.value[uploaded_filename]['content']   \n",
    "        output_file.write(content)\n",
    "    \n",
    "    print(f\"----Saved audiobook at {audiobook_path}\")\n",
    "    \n",
    "    return audiobook_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-effectiveness",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Creating a temperory directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-ethnic",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def createDir(file_path):\n",
    "    file_name = (file_path.split(\"\\\\\")[-1])\n",
    "    file_name = file_name.split(\"/\")[-1][:-4]\n",
    "    creation_time = time.time()\n",
    "\n",
    "    paths = {\n",
    "        \"audio_model\": path(\"../assets/audio_sentiment_data_v2/models/hyperband_tuned_model_final_[0.260879248380661, 0.9069767594337463]/\"),\n",
    "        \"pickles\": path(\"../assets/audio_sentiment_data_v2/pickles\"),\n",
    "        \"text_model\": path(\"../assets/text_sentiment_data/models/bert_model_2/\"),\n",
    "        \"music_model\": path(\"../assets/music_generation/models/MusicTransformerKeyC.pth\"),\n",
    "        \"music_data\": path(\"../assets/music_generation/pickles/\"),\n",
    "        \"wav_save_path\": path(f\"../assets/temp/{file_name}-{creation_time}\"),\n",
    "        \"clips_save_path\": path(f\"../assets/temp/{file_name}-{creation_time}/clips\"),\n",
    "        \"music_samples\": path(\"../assets/music_generation/datasets/vg-midi-annotated\")\n",
    "    }\n",
    "    \n",
    "    # Creating directories in temp to store the converted wav file and the clips\n",
    "    os.mkdir(paths[\"wav_save_path\"])\n",
    "    os.mkdir(paths[\"clips_save_path\"])\n",
    "    \n",
    "    print(\"----Temporary directory created.\")\n",
    "    \n",
    "    return file_name, paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-monday",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Loading assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-stationery",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def loadAssets(paths):\n",
    "    # Loading the audio analyzer model, scaler and classes\n",
    "    current_time = time.time()\n",
    "    audio_model, audio_scaler, audio_classes = loadAudioAssets(\n",
    "        model_path=paths[\"audio_model\"], pickles_path=paths[\"pickles\"]\n",
    "    )\n",
    "    print(f\"----Loaded audio model assets. Time taken: {round(time.time()-current_time, 1)} s\")\n",
    "\n",
    "    # Loading the text analyzer model and classes\n",
    "    current_time = time.time()\n",
    "    text_predictor = ktrain.load_predictor(paths[\"text_model\"])\n",
    "    text_classes = text_predictor.get_classes()\n",
    "    print(f\"----Loaded text model assets. Time taken: {round(time.time()-current_time, 1)} s\")\n",
    "    \n",
    "    # Loading the music generation model and music_data\n",
    "    current_time = time.time()\n",
    "    music_data = load_data(paths['music_data'], 'musicitem_data_save.pkl')\n",
    "    music_model = music_model_learner(music_data, pretrained_path=paths['music_model'])\n",
    "    print(f\"----Loaded music model assets. Time taken: {round(time.time()-current_time, 1)} s\")\n",
    "    \n",
    "    # Setting up IBM\n",
    "    current_time = time.time()\n",
    "    stt = setUpIBM()\n",
    "    print(f\"----Setup IBM transcription service. Time taken: {round(time.time()-current_time, 1)} s\")\n",
    "    \n",
    "    assets = {'audio_model': audio_model,\n",
    "              'audio_scaler': audio_scaler,\n",
    "              'audio_classes': audio_classes,\n",
    "              'text_predictor': text_predictor,\n",
    "              'text_classes': text_classes,\n",
    "              'music_data': music_data,\n",
    "              'music_model': music_model,\n",
    "              'stt': stt}\n",
    "    \n",
    "    return assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-might",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Performing text and audio sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-authorization",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def sentimentAnalysis(paths, stt, text_predictor, audio_model, audio_scaler, audio_classes):\n",
    "    transcriptions = [] # List to store transcriptions of all the segmented clips\n",
    "    emotions = [] # List to store emotions of all the segmented clips\n",
    "    \n",
    "    for i, file_name in enumerate(glob.glob(f'{paths[\"clips_save_path\"]}/*.wav')):\n",
    "        current_time = time.time()\n",
    "        print(f\"\\nProcessing clip {i+1}:\")\n",
    "\n",
    "        # Performing text sentiment analysis\n",
    "        print(\"----Text sentiment analysis\") \n",
    "        text_emotions, transcription = analyzeText(\n",
    "            file_name=file_name, stt=stt, predictor=text_predictor\n",
    "        )\n",
    "        \n",
    "        # Performing text sentiment analysis\n",
    "        print(\"----Audio sentiment analysis\")\n",
    "        audio_emotions = analyzeAudio(\n",
    "            file_name=file_name, model=audio_model, scaler=audio_scaler\n",
    "        )\n",
    "        \n",
    "        # Taking the average of text and audio emotions\n",
    "        print(\"----Predicting final emotion\")\n",
    "        weighted_text_emotions = text_emotions * 0.5\n",
    "        weighted_audio_emotions = audio_emotions * 0.5\n",
    "        weighted_emotions = weighted_text_emotions + weighted_audio_emotions\n",
    "\n",
    "        # Picking the dominant emotion and labelling it\n",
    "        weighted_emotions = weighted_emotions.argmax()\n",
    "        weighted_emotions = weighted_emotions.astype(int).flatten()\n",
    "        final_emotion = audio_classes.inverse_transform((weighted_emotions))\n",
    "\n",
    "        transcriptions.append(transcription)\n",
    "        emotions.append(final_emotion)\n",
    "        \n",
    "        print(f\"----Clip {i+1} processed. Time taken: {round(time.time()-current_time, 1)} s\")\n",
    "        \n",
    "    return transcriptions, emotions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def musicClipsGeneration(paths, music_model, music_data, songs, music_emotions=['Angry', 'Happy', 'Neutral', 'Sad']):\n",
    "    # Generating music for each emotion\n",
    "    for music_emotion in music_emotions:\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Generating a song\n",
    "        full_song = generateMusic(music_emotion, \n",
    "                                  f\"{paths['music_samples']}\", \n",
    "                                  music_model,\n",
    "                                  music_data)\n",
    "        \n",
    "        print(f\"----generated {music_emotion} clip. Time taken: {round(time.time()-current_time, 1)} s\")\n",
    "        \n",
    "        # Adding the song created for the emotion to a dictionary\n",
    "        songs[music_emotion] = full_song\n",
    "    \n",
    "    return songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-battery",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "## Main Function\n",
    "**Input:** Audiobook in mp3 format.  \n",
    "\n",
    "**Procedure:**\n",
    "- The audiobook is segmented into 30 second clips.\n",
    "- Each segment is analyzed by the audio and text analyzers.\n",
    "- A sentiment is predicted for each segment of the audiobook.\n",
    "- Music clips are generated for the four emotions (Angry, Happy, Neutral and Sad) \n",
    "\n",
    "**Output:**\n",
    "- Audiobook segments with its transcription and predicted emotion\n",
    "- Music clips for each emotion (Angry, Happy, Neutral and Sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-zealand",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class deepAudiobookTuner:    \n",
    "    def __init__(self, audiobook_path):\n",
    "        self.audiobook_path = audiobook_path\n",
    "        \n",
    "        # Creating a temperory directory to store the segmented audiobook clips and generated music clips\n",
    "        print(\"\\nCreating temporary directory.\")\n",
    "        self.file_name, self.paths = createDir(audiobook_path)\n",
    "\n",
    "        # Loading assets.\n",
    "        print(\"\\nLoading assets.\")\n",
    "        self.assets = loadAssets(self.paths)\n",
    "    \n",
    "        # Converting the mp3 file to a wav file\n",
    "        print(\"\\nConverting mp3 to wav\")\n",
    "        self.wav_file_path = convertToWav(\n",
    "            file_name = self.file_name, file_path = self.audiobook_path, save_path = self.paths[\"wav_save_path\"]\n",
    "        )\n",
    "\n",
    "        # Segmenting the audio file into 30 second clips\n",
    "        print(\"\\nSegmenting audiobook\")\n",
    "        segmentAudioFile(\n",
    "            file_name = self.file_name, file_path = self.wav_file_path, save_path = self.paths[\"clips_save_path\"]\n",
    "        )\n",
    "        \n",
    "        self.songs = {}\n",
    "        \n",
    "    \n",
    "    def analyzeSentiments(self):\n",
    "        # Performing Sentiment Analysis\n",
    "        print(\"\\n\\nPerforming sentiment analysis\")\n",
    "        self.transcriptions, self.emotions = sentimentAnalysis(self.paths,\n",
    "                                                               self.assets['stt'], \n",
    "                                                               self.assets['text_predictor'],\n",
    "                                                               self.assets['audio_model'],\n",
    "                                                               self.assets['audio_scaler'],\n",
    "                                                               self.assets['audio_classes'])\n",
    "    \n",
    "    def generateMusicClips(self):\n",
    "        # Generating music\n",
    "        print(\"\\n\\nGenerating music\")\n",
    "        self.songs = musicClipsGeneration(paths = self.paths, \n",
    "                                          music_model = self.assets['music_model'], \n",
    "                                          music_data = self.assets['music_data'], \n",
    "                                          songs = self.songs)\n",
    "           \n",
    "        \n",
    "    def regenerateMusicClips(self, music_emotions):\n",
    "        # Generating music\n",
    "        print(\"\\n\\nRegenerating music\")\n",
    "        self.songs = musicClipsGeneration(music_emotions = music_emotions,\n",
    "                                          paths = self.paths, \n",
    "                                          music_model = self.assets['music_model'], \n",
    "                                          music_data = self.assets['music_data'], \n",
    "                                          songs = self.songs)\n",
    "        \n",
    "    def displayOutputs(self):\n",
    "        print(\"\\nTranscriptions and Emotions:\")\n",
    "        for i, (transcription, emotion) in enumerate(zip(self.transcriptions, self.emotions)):\n",
    "            print(f\"\\nClip {i+1}:\")\n",
    "            print(f\"Transcription: {transcription}\")\n",
    "            print(f\"Emotion: {emotion}\")\n",
    "        \n",
    "        print(\"\\n\\nGenerated Music:\")\n",
    "        for music_emotion in self.songs:\n",
    "            print(f\"\\nMusic clip for emotion: {music_emotion}:\")\n",
    "            music_clip = self.songs[music_emotion].stream\n",
    "            music_clip.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-wagon",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def main(audiobook_path):\n",
    "    current_time = time.time()\n",
    "\n",
    "    tuner = deepAudiobookTuner(audiobook_path)\n",
    "    tuner.analyzeSentiments()\n",
    "    tuner.generateMusicClips()\n",
    "\n",
    "    print(f\"\\n\\nJob complete. Total time taken: {round(time.time()-current_time, 1)} s\")\n",
    "    print(\"\\n\\n-----------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "    print(\"Outputs: \")\n",
    "\n",
    "    tuner.displayOutputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-strain",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "\n",
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-package",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking an audiobook in mp3 format as the input\n",
    "print(\"\\nUpload an audiobook in mp3 format.\\n\")\n",
    "audiobook_path = asyncio.ensure_future(uploadAudiobook())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-burton",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "main(audiobook_path.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "deepaudiobooktuner",
   "language": "python",
   "name": "deepaudiobooktuner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
